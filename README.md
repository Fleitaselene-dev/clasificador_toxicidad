# üß† Clasificador de Toxicidad

Este es un peque√±o proyecto web que utiliza [TensorFlow.js](https://www.tensorflow.org/js) y el modelo preentrenado [`toxicity`](https://github.com/tensorflow/tfjs-models/tree/master/toxicity) para detectar contenido t√≥xico en frases escritas en ingl√©s.

## ¬øQu√© hace?

- Permite al usuario ingresar una frase en ingl√©s.
- Al hacer clic en el bot√≥n "Analizar", el modelo detecta si la frase contiene:
  - Insultos
  - Lenguaje obsceno
  - Discriminaci√≥n
  - Amenazas
  - Comentarios sexuales inapropiados
  - Entre otros tipos de toxicidad
- Muestra el resultado en formato JSON con los an√°lisis correspondientes.

## ¬øC√≥mo ejecutarlo?

1. Clon√° o descarg√° este repositorio.
   ```bash
   cd https://github.com/Fleitaselene-dev/clasificador_toxicidad.git
3. Abr√≠ el archivo `index2.html` en tu navegador.
4. Escrib√≠ una frase en ingl√©s y presion√° "Analizar".


## üì¶ Tecnolog√≠as utilizadas

- HTML5
- CSS3
- JavaScript
- [TensorFlow.js](https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.0.0/)
- [Modelo de Toxicidad](https://cdn.jsdelivr.net/npm/@tensorflow-models/toxicity)

